{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='uncertainty.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"../errortools/\")\n",
    "import errortools\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing\n",
    "import random\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "SMALL_SIZE = 20\n",
    "MEDIUM_SIZE = 24\n",
    "BIGGER_SIZE = 28\n",
    "\n",
    "matplotlib.rc('font', size=MEDIUM_SIZE)         # controls default text sizes\n",
    "matplotlib.rc('axes', titlesize=MEDIUM_SIZE)    # fontsize of the axes title\n",
    "matplotlib.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "matplotlib.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "matplotlib.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "matplotlib.rc('legend', fontsize=SMALL_SIZE)   # legend fontsize\n",
    "matplotlib.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "matplotlib.rcParams['font.family'] = 'serif'\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "def Print(string):\n",
    "    display(Markdown(string))\n",
    "    \n",
    "np.random.seed(42)\n",
    "np.set_printoptions(precision=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "_Uncertainty/error_ = __how far__ we could be off in our prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Roughly two sources of uncertainty  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Model, assumptions, features, processing, ...  \n",
    "\n",
    "Sometimes called _systematic uncertainties_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is not about those"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Limited training data  \n",
    "\n",
    "Sometimes called _statistical uncertainties_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is about those"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# An illustrative example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We create a dataset according to a perfectly known sigmoid probability distribution  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidePrompt": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "slope = 4 \n",
    "bias  = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true,
    "hide_input": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(20, 5))\n",
    "x = np.linspace(-1, 1, 101)\n",
    "p = scipy.stats.logistic.cdf(slope * x + bias)\n",
    "ax.plot(x, p, '-', color='black', alpha=1)\n",
    "ax.set_xlabel(\"some variable X\")\n",
    "ax.set_ylabel(\"probability\")\n",
    "ax.grid()\n",
    "ax.set_ylim((0,1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidePrompt": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "n_traindata = 100\n",
    "slope = 4 \n",
    "bias  = 0\n",
    "\n",
    "X = np.random.uniform(low=-1, high=1, size=n_traindata)\n",
    "p = scipy.stats.logistic.cdf(X * slope + bias)\n",
    "y = (p > np.random.uniform(size=n_traindata)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true,
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "H, e = np.histogram(X, bins=10, range=(-1,1))\n",
    "h, e = np.histogram(X[y==1], bins=10, range=(-1,1))\n",
    "r = h/(H+1e-12)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 5))\n",
    "x = np.linspace(-1, 1, 101)\n",
    "p = scipy.stats.logistic.cdf(slope * x + bias)\n",
    "ax.plot(X[y==0], y[y==0], 'o', color='red', markersize=10, label=\"0s\")\n",
    "ax.plot(X[y==1], y[y==1], 'X', color='green', markersize=10, label=\"1s\")\n",
    "ax.plot(x, p, '--', color='black', alpha=0.8)\n",
    "#ax.bar((e[:-1]+e[1:])/2., r, e[1]-e[0], color=\"orange\", alpha=0.5, label=\"fraction of 1s\")\n",
    "ax.set_xlabel(\"some variable X\")\n",
    "ax.set_ylabel(\"probability\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "ax.set_ylim((-0.1,1.1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We fit a logistic regression to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "model = errortools.LogisticRegression()\n",
    "model.fit(X, y)\n",
    "print(model.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "H, e = np.histogram(X, bins=10, range=(-1,1))\n",
    "h, e = np.histogram(X[y==1], bins=10, range=(-1,1))\n",
    "r = h/(H+1e-12)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 5))\n",
    "x = np.linspace(-1, 1, 101)\n",
    "p = scipy.stats.logistic.cdf(slope * x + bias)\n",
    "f = model.predict(x)\n",
    "ax.plot(x, p, '--', color='black', alpha=1, label=\"true probability: slope {:.0f} bias {:.0f}\".format(slope, bias))\n",
    "#ax.bar((e[:-1]+e[1:])/2., r, e[1]-e[0], color=\"orange\", alpha=0.1)\n",
    "ax.plot(x, f, '-', color='red', alpha=1, label=\"model: slope {:.1f} bias {:.1f}\".format(model.parameters[0], model.parameters[1]))\n",
    "ax.set_xlabel(\"some variable X\")\n",
    "ax.set_ylabel(\"probability\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "ax.set_ylim((0,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our model does not get back the exact slope and bias that we put in  \n",
    "\n",
    "The reason is the dataset  \n",
    "\n",
    "If we had a different dataset, we would get different values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's illustrate this  \n",
    "We repeat the example many times  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "n_datasets = 1000\n",
    "Xs = np.random.uniform(low=-1, high=1, size=(n_datasets, n_traindata))\n",
    "ps = scipy.stats.logistic.cdf(Xs * slope + bias)\n",
    "ys = (ps > np.random.uniform(size=Xs.shape)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "for i in range(n_datasets):\n",
    "    m = errortools.LogisticRegression()\n",
    "    m.fit(Xs[i], ys[i])\n",
    "    models.append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true,
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15,2))\n",
    "ax[0].hist([m.parameters[0] for m in models], bins=40, range=(2,7), color='orange', alpha=0.5, label=\"models\")\n",
    "ax[0].plot((slope, slope), ax[0].get_ylim(), color='black', label=\"True value\")\n",
    "ax[0].grid()\n",
    "ax[0].set_xlabel(\"Slope\")\n",
    "ax[0].set_ylabel(\"Count\")\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].hist([m.parameters[1] for m in models], bins=40, range=(-1,1), color='orange', alpha=0.5, label=\"models\")\n",
    "ax[1].plot((bias, bias), ax[1].get_ylim(), color='black', label=\"True value\")\n",
    "ax[1].grid()\n",
    "ax[1].set_xlabel(\"Bias\")\n",
    "ax[1].legend()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15,3))\n",
    "p = scipy.stats.logistic.cdf(slope * x + bias)\n",
    "ax.plot(x, models[0].predict(x), '-', color='orange', alpha=0.25, label=\"models\")\n",
    "for m in models[1:]:\n",
    "    ax.plot(x, m.predict(x), '-', color='orange', alpha=0.1)\n",
    "ax.plot(x, p, '--', color='black', label=\"Real probability\")\n",
    "ax.set_ylim((0,1))\n",
    "ax.set_xlabel(\"Some variable X\")\n",
    "ax.set_ylabel(\"Probability\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### We see that\n",
    "\n",
    "A model's parameters and prediction curves depend on the training data  \n",
    "\n",
    "* And thereby also the AUC, confusion matrix, recall, precision, etc.  \n",
    "\n",
    "We thus have an uncertainty on our predictions  \n",
    "\n",
    "Can we somehow estimate these uncertainties?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Estimating uncertainties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Two steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "1. Estimate uncertainties on model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "2. Propagate uncertainties to predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Estimate uncertainties on model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In many cases we minimize a loss function $L_{oss}(p; X, y)$  \n",
    "\n",
    "__And implicitly maximise a likelihood__ $L(p|X,y)\\sim e^{-L_{oss}(p; X, y)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The optimal model parameters\n",
    "* minimize the loss function\n",
    "* are the most likely parameters for the given dataset \n",
    "\n",
    "But surrounding parameter values are still likely  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "Z = np.concatenate((X[:, np.newaxis], np.ones((X.shape[0], 1))), axis=1)\n",
    "f0 = model.negativeLogPosterior(model.parameters, Z, y, 0, 0)\n",
    "\n",
    "p = np.tile(model.parameters, (101,1))\n",
    "p[:, 0] += np.linspace(-2, 2, 101)\n",
    "loss = np.exp( -np.array([model.negativeLogPosterior(q, Z, y, 0, 0) for q in p]) )\n",
    "\n",
    "ax[0].plot(p[:, 0], loss, '-', color='orange')\n",
    "ax[0].grid()\n",
    "ax[0].set_xlabel(\"slope\")\n",
    "ax[0].set_ylabel(\"likelihood\");\n",
    "\n",
    "p = np.tile(model.parameters, (101,1))\n",
    "p[:, 1] += np.linspace(-1, 1, 101)\n",
    "loss = np.exp( -np.array([model.negativeLogPosterior(q, Z, y, 0, 0) for q in p]) )\n",
    "\n",
    "ax[1].plot(p[:, 1], loss, '-', color='orange')\n",
    "ax[1].grid()\n",
    "ax[1].set_xlabel(\"bias\");\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We make a parabolic approximation of the loss function  \n",
    "$L_{oss}(p; X, y)\\approx L_{oss}(p_0) + \\frac{1}{2}(p-p_0)\\cdot\\frac{\\partial^2 L_{oss}}{\\partial p^2}\\cdot(p-p_0)$  \n",
    "\n",
    "* This turns the likelihood into a multivariate Gaussian distribution  \n",
    "* $L(p|X,y)\\approx e^{-\\frac{1}{2}(p-p_0)\\cdot\\frac{\\partial^2 L_{oss}}{\\partial p^2}\\cdot(p-p_0)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "hessian = scipy.linalg.inv(model.cvr_mtx)\n",
    "Z = np.concatenate((X[:, np.newaxis], np.ones((X.shape[0], 1))), axis=1)\n",
    "f0 = model.negativeLogPosterior(model.parameters, Z, y, 0, 0)\n",
    "\n",
    "p = np.tile(model.parameters, (101,1))\n",
    "p[:, 0] += np.linspace(-model.errors[0], model.errors[0], 101)\n",
    "approx = np.exp( -np.array([f0 + 0.5*np.dot(a-model.parameters, np.dot(hessian, a-model.parameters)) for a in p]) )\n",
    "\n",
    "#ax[0].fill_between(x=p[:,0], y1=approx, color='red', hatch=\"x\", alpha=0.05, label=\"86%\")\n",
    "\n",
    "p = np.tile(model.parameters, (101,1))\n",
    "p[:, 0] += np.linspace(-2, 2, 101) \n",
    "loss = np.exp( -np.array([model.negativeLogPosterior(q, Z, y, 0, 0) for q in p]) )\n",
    "approx = np.exp( -np.array([f0 + 0.5*np.dot(a-model.parameters, np.dot(hessian, a-model.parameters)) for a in p]) )\n",
    "\n",
    "ax[0].plot(p[:, 0], loss, '-', color='orange', label=\"likelihood\")\n",
    "ax[0].plot(p[:, 0], approx, '--', color='red', label=\"Gaussian approximation\", alpha=0.5)\n",
    "ax[0].grid()\n",
    "ax[0].set_xlabel(\"slope\")\n",
    "ax[0].set_ylabel(\"likelihood\")\n",
    "\n",
    "p = np.tile(model.parameters, (101,1))\n",
    "p[:, 1] += np.linspace(-model.errors[1], model.errors[1], 101)\n",
    "approx = np.exp( -np.array([f0 + 0.5*np.dot(a-model.parameters, np.dot(hessian, a-model.parameters)) for a in p]) )\n",
    "\n",
    "#ax[1].fill_between(x=p[:,1], y1=approx, color='red', hatch=\"x\", alpha=0.05, label=\"68%\")\n",
    "\n",
    "p = np.tile(model.parameters, (101,1))\n",
    "p[:, 1] += np.linspace(-1, 1, 101)\n",
    "loss = np.exp( -np.array([model.negativeLogPosterior(q, Z, y, 0, 0) for q in p]) )\n",
    "approx = np.exp( -np.array([f0 + 0.5*np.dot(a-model.parameters, np.dot(hessian, a-model.parameters)) for a in p]) )\n",
    "\n",
    "ax[1].plot(p[:, 1], loss, '-', color='orange', label=\"likelihood\")\n",
    "ax[1].plot(p[:, 1], approx, '--', color='red', label=\"Gaussian approximation\", alpha=0.5)\n",
    "ax[1].grid()\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel(\"bias\");\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The approximation automatically gives us the parameter errors.\n",
    "They are given by the covariance matrix  \n",
    "$\\hat{\\Sigma}_{p} = \\left[\\frac{\\partial^2 L_{oss}}{\\partial p^2}\\right]^{-1}$  \n",
    "\n",
    "* Diagonal elements give the parameter errors\n",
    "* Off-diagonal elements give the parameter correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "hessian = scipy.linalg.inv(model.cvr_mtx)\n",
    "Z = np.concatenate((X[:, np.newaxis], np.ones((X.shape[0], 1))), axis=1)\n",
    "f0 = model.negativeLogPosterior(model.parameters, Z, y, 0, 0)\n",
    "\n",
    "p = np.tile(model.parameters, (101,1))\n",
    "p[:, 0] += np.linspace(-model.errors[0], model.errors[0], 101)\n",
    "approx = np.exp( -np.array([f0 + 0.5*np.dot(a-model.parameters, np.dot(hessian, a-model.parameters)) for a in p]) )\n",
    "\n",
    "ax[0].fill_between(x=p[:,0], y1=approx, color='red', hatch=\"x\", alpha=0.05, label=\"68%\")\n",
    "\n",
    "p = np.tile(model.parameters, (101,1))\n",
    "p[:, 0] += np.linspace(-2, 2, 101) \n",
    "loss = np.exp( -np.array([model.negativeLogPosterior(q, Z, y, 0, 0) for q in p]) )\n",
    "approx = np.exp( -np.array([f0 + 0.5*np.dot(a-model.parameters, np.dot(hessian, a-model.parameters)) for a in p]) )\n",
    "\n",
    "ax[0].plot(p[:, 0], loss, '-', color='orange', label=\"likelihood\")\n",
    "ax[0].plot(p[:, 0], approx, '--', color='red', label=\"Gaussian approximation\", alpha=0.5)\n",
    "ax[0].grid()\n",
    "ax[0].set_xlabel(\"slope\")\n",
    "ax[0].set_ylabel(\"likelihood\")\n",
    "\n",
    "p = np.tile(model.parameters, (101,1))\n",
    "p[:, 1] += np.linspace(-model.errors[1], model.errors[1], 101)\n",
    "approx = np.exp( -np.array([f0 + 0.5*np.dot(a-model.parameters, np.dot(hessian, a-model.parameters)) for a in p]) )\n",
    "\n",
    "ax[1].fill_between(x=p[:,1], y1=approx, color='red', hatch=\"x\", alpha=0.05, label=\"68%\")\n",
    "\n",
    "p = np.tile(model.parameters, (101,1))\n",
    "p[:, 1] += np.linspace(-1, 1, 101)\n",
    "loss = np.exp( -np.array([model.negativeLogPosterior(q, Z, y, 0, 0) for q in p]) )\n",
    "approx = np.exp( -np.array([f0 + 0.5*np.dot(a-model.parameters, np.dot(hessian, a-model.parameters)) for a in p]) )\n",
    "\n",
    "ax[1].plot(p[:, 1], loss, '-', color='orange', label=\"likelihood\")\n",
    "ax[1].plot(p[:, 1], approx, '--', color='red', label=\"Gaussian approximation\", alpha=0.5)\n",
    "ax[1].grid()\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel(\"bias\");\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The _Minuit_ minimisation package automatically calculates the parameter covariance matrix  \n",
    "* _Minuit_ is a robust, age tested minimisation package, used extensively in particle physics\n",
    "* It is available in Python through the `iminuit` package\n",
    "* We use `iminuit` where possible for its robustness and its covariance matrix calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Propagate uncertainties to predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A prediction is a function of input features $X$ and model parameters $p$  \n",
    " * $f(X|p) = \\frac{1}{1+e^{-X\\cdot p}}$  \n",
    " \n",
    "The most likely model parameters $p_0$ determine the prediction  \n",
    "\n",
    "And the uncertainties in the parameters propagate to an uncertainty in the prediction \n",
    "* Multiple ways to propagate uncertainties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Linear error propagation**  \n",
    "   * Linear approximation of the prediction function\n",
    "       * $f(X|p)\\approx f(X|p_0)+\\frac{\\partial f}{\\partial p}(X|p_0)\\cdot(p-p_0)$\n",
    "   * Makes the prediction uncertainty a simple equation\n",
    "       * $\\Delta f \\approx  \\sqrt{\\frac{\\partial f}{\\partial p}(X|p_0) \\cdot \\hat{\\Sigma}_p \\cdot \\frac{\\partial f}{\\partial p}(X|p_0)}$  \n",
    "       \n",
    "Fast calculation, but may be inexact    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Sample the Gaussian likelihood**  \n",
    "* Take random parameters $p$ from $L(p|X,y)\\approx e^{-\\frac{1}{2}(p-p_0)\\cdot\\hat{\\Sigma}_{p}^{-1}\\cdot(p-p_0)}$ \n",
    "* Calculate $E_p\\left[ \\left(f(X|p)-f(X|p_0)\\right)^2 \\right]$  \n",
    "\n",
    "Slower, but more exact with more samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "hessian = scipy.linalg.inv(model.cvr_mtx)\n",
    "Z = np.concatenate((X[:, np.newaxis], np.ones((X.shape[0], 1))), axis=1)\n",
    "f0 = model.negativeLogPosterior(model.parameters, Z, y, 0, 0)\n",
    "\n",
    "p = np.tile(model.parameters, (101,1))\n",
    "p[:, 0] += np.linspace(-model.errors[0], model.errors[0], 101)\n",
    "approx = np.exp( -np.array([f0 + 0.5*np.dot(a-model.parameters, np.dot(hessian, a-model.parameters)) for a in p]) )\n",
    "\n",
    "#ax[0].fill_between(x=p[:,0], y1=approx, color='red', hatch=\"x\", alpha=0.05, label=\"86%\")\n",
    "\n",
    "p = np.tile(model.parameters, (101,1))\n",
    "p[:, 0] += np.linspace(-2, 2, 101) \n",
    "loss = np.exp( -np.array([model.negativeLogPosterior(q, Z, y, 0, 0) for q in p]) )\n",
    "approx = np.exp( -np.array([f0 + 0.5*np.dot(a-model.parameters, np.dot(hessian, a-model.parameters)) for a in p]) )\n",
    "\n",
    "#ax[0].plot(p[:, 0], loss, '-', color='orange', label=\"likelihood\")\n",
    "ax[0].plot(p[:, 0], approx, '-', color='red', label=\"Gaussian approximation\", alpha=0.5)\n",
    "ax[0].grid()\n",
    "ax[0].set_xlabel(\"slope\")\n",
    "ax[0].set_ylabel(\"likelihood\")\n",
    "\n",
    "p = np.tile(model.parameters, (101,1))\n",
    "p[:, 1] += np.linspace(-model.errors[1], model.errors[1], 101)\n",
    "approx = np.exp( -np.array([f0 + 0.5*np.dot(a-model.parameters, np.dot(hessian, a-model.parameters)) for a in p]) )\n",
    "\n",
    "#ax[1].fill_between(x=p[:,1], y1=approx, color='red', hatch=\"x\", alpha=0.05, label=\"68%\")\n",
    "\n",
    "p = np.tile(model.parameters, (101,1))\n",
    "p[:, 1] += np.linspace(-1, 1, 101)\n",
    "loss = np.exp( -np.array([model.negativeLogPosterior(q, Z, y, 0, 0) for q in p]) )\n",
    "approx = np.exp( -np.array([f0 + 0.5*np.dot(a-model.parameters, np.dot(hessian, a-model.parameters)) for a in p]) )\n",
    "\n",
    "#ax[1].plot(p[:, 1], loss, '-', color='orange', label=\"likelihood\")\n",
    "ax[1].plot(p[:, 1], approx, '-', color='red', label=\"Gaussian approximation\", alpha=0.5)\n",
    "ax[1].grid()\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel(\"bias\");\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Uncertainties in action\n",
    "\n",
    "Surviving the Titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Get information on Titanic passengers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"http://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Do some data mangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "df['sex'] = df.Sex.apply(lambda s: 1 if s=='female' else 0)\n",
    "df['has siblings or spouses aboard'] = (df['Siblings/Spouses Aboard'] > 0).astype(int)\n",
    "df['has parents or children aboard'] = (df['Parents/Children Aboard'] > 0).astype(int)\n",
    "df['has family aboard'] = (df['Siblings/Spouses Aboard'] + df['Parents/Children Aboard'] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at some data distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hidePrompt": true,
    "hide_input": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dfsurv = df[df.Survived==1]\n",
    "dfdied = df[df.Survived==0]\n",
    "fig, ax = plt.subplots(2, 3, figsize=(20, 12))\n",
    "ax[0,0].hist(df.Pclass, bins=3, range=(0.5,3.5), color='orange', alpha=0.5, label=\"all\")\n",
    "ax[0,0].hist(dfsurv.Pclass, bins=3, range=(0.5,3.5), color='green', alpha=0.5, label=\"survived\")\n",
    "ax[0,0].set_xlabel(\"Pclass\")\n",
    "ax[0,0].legend()\n",
    "\n",
    "ax[0,1].hist(df.Age, bins=16, range=(0,80), color='orange', alpha=0.5, label=\"all\")\n",
    "ax[0,1].hist(dfsurv.Age, bins=16, range=(0,80), color='green', alpha=0.5, label=\"survived\")\n",
    "ax[0,1].set_xlabel(\"Age\")\n",
    "\n",
    "ax[0,2].hist(df['Siblings/Spouses Aboard'], bins=9, range=(-0.5,8.5), color='orange', alpha=0.5, label=\"all\")\n",
    "ax[0,2].hist(dfsurv['Siblings/Spouses Aboard'], bins=9, range=(-0.5,8.5), color='green', alpha=0.5, label=\"survived\")\n",
    "ax[0,2].set_xlabel(\"Siblings/Spouses Aboard\")\n",
    "\n",
    "ax[1,0].hist(df['Parents/Children Aboard'], bins=7, range=(-0.5,6.5), color='orange', alpha=0.5, label=\"all\")\n",
    "ax[1,0].hist(dfsurv['Parents/Children Aboard'], bins=7, range=(-0.5,6.5), color='green', alpha=0.5, label=\"survived\")\n",
    "ax[1,0].set_xlabel(\"Parents/Children Aboard\")\n",
    "\n",
    "ax[1,1].hist(df['Fare'], bins=20, range=(0,200), color='orange', alpha=0.5, label=\"all\")\n",
    "ax[1,1].hist(dfsurv['Fare'], bins=20, range=(0,200), color='green', alpha=0.5, label=\"survived\")\n",
    "ax[1,1].set_xlabel(\"Fare\")\n",
    "\n",
    "ax[1,2].hist(df['sex'], bins=2, range=(-0.5,1.5), color='orange', alpha=0.5, label=\"all\")\n",
    "ax[1,2].hist(dfsurv['sex'], bins=2, range=(-0.5,1.5), color='green', alpha=0.5, label=\"survived\")\n",
    "ax[1,2].set_xlabel(\"sex\")\n",
    "ax[1,2].get_xaxis().set_ticks([0,1])\n",
    "ax[1,2].get_xaxis().set_ticklabels(['male', 'female']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's train a model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df_train = df.sample(frac=0.5, replace=False, random_state=42)\n",
    "df_test  = df.loc[df.index.difference(df_train.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "features = ['Pclass', 'Age', 'Fare', 'Siblings/Spouses Aboard', \n",
    "            'Parents/Children Aboard', 'sex']\n",
    "target = 'Survived'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hideOutput": false,
    "hidePrompt": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "X_train = df_train[features].values\n",
    "y_train = df_train[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": true,
    "hideOutput": false,
    "hidePrompt": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "X_test = df_test[features].values\n",
    "y_test = df_test[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "titanic_model = errortools.LogisticRegression(fit_intercept=True)\n",
    "titanic_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Uncertainty over the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "errortools.report_parameter_error(titanic_model, \n",
    "                                  features + ['bias'], \n",
    "                                  figsize=(20, 4), \n",
    "                                  rotation_x_labels=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What happens when we scale the data with a min-max scaler?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "scaler = sklearn.preprocessing.MinMaxScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "titanic_model.fit(X_train_scaled, y_train)\n",
    "errortools.report_parameter_error(titanic_model, \n",
    "                                  features + ['bias'], \n",
    "                                  figsize=(20, 4), \n",
    "                                  rotation_x_labels=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How about the uncertainty over our test samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "X_test_scaled = scaler.transform(X_test)\n",
    "scores_test = titanic_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "errortools.report_error_test_samples(titanic_model, \n",
    "                                     X_test_scaled,\n",
    "                                     figsize=(20, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Uncertainties over individual predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_model.fit(X_train, y_train)\n",
    "errortools.report_error_indivial_pred(titanic_model, \n",
    "                                      X_test[random.randint(0, len(X_test))], \n",
    "                                      'Fare', features, 0, 1000, 100,\n",
    "                                      figsize=(20,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Systemic error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errortools.report_model_positive_ratio(titanic_model, X_test_scaled, y_test, 1000, 10, figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "livereveal": {
   "controls": false,
   "height": 768,
   "scroll": true,
   "theme": "serif",
   "transition": "linear",
   "width": 1024
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
